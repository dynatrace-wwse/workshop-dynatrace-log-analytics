{
    "version": "6",
    "defaultTimeframe": {
        "from": "now()-2h",
        "to": "now()"
    },
    "defaultSegments": [],
    "sections": [
        {
            "id": "22e905c4-ce9e-4a88-a9cb-1141543c0ef3",
            "type": "markdown",
            "markdown": "# Using Dynatrace OpenPipeline by Grail for processing rules and metric extraction  \n#\n## Overview - What Youâ€™ll Learn Today \n#\n1. Create new storage bucket(s)\n2. Create DPL to extract important data to fields\n3. Create a new pipeline for AstroShop, apply the DPL transformation\n4. Create a Business Event for AstroShop add to cart business function\n5. Create a Davis Event extraction for PaymentService anomaly detection\n6. Create a Metric extraction for logs by log level\n7. Create a Bucket assignment rule\n8. Create a Dynamic Route for new pipeline\n9. Verify pipeline created and metric extraction is working\n\n#\nDynatrace Log Management and Analytics, powered by Grail, provides a unified approach to unlocking the value of log data in the Dynatrace platform. Hassle-free management of your log data lets you store petabytes of data without schemas, indexing, or rehydration. All of that data is usable at any time for any analytics task. Thanks to schema on-read and the Dynatrace Query Language, there's no need to decide what you want to query during data ingestion. Pick the retention period for your data that fits your business and compliance needs, whether debugging or audit.\n#\nOpenPipeline is the Dynatrace data handling solution to seamlessly ingest and process data from different sources, at any scale, and in any format in the Dynatrace Platform.\n\n### Use cases\n* Configure ingestion and processing of different data types, such as logs and events, via a unified solution.\n* Scala data management across teams by controlling access and targeted technologies.\n* Ensure secure and compliant sensitive data handling.\n* Enrich and contextualize data via customizable data processing.\n* Optimize data quality and cost control.\n#\nWith OpenPipeline, you can ingest data in the Dynatrace platform from a wide variety of formats and providers, through ingest sources. Data is then routed to pipelines for processing, and stored in Grail buckets.\n#\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_openpipeline_diagram.png)\n\n\n Reference: [OpenPipeline](https://docs.dynatrace.com/docs/shortlink/openpipeline-dataflow)"
        },
        {
            "id": "db475246-3b51-48ce-b847-d973b6d3eb27",
            "type": "markdown",
            "markdown": "#### Step 1: Create new storage bucket(s)\n\nGo to Settings > Storage Management > Bucket Storage Management > `+ Bucket`\n\nCreate a new bucket, using the following configurations:\n\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_storage_management_new_bucket.png)\n\n**AstroShop Log Buckets**\n\nPaymentService Logs for 365 Days\n\n| **Field**                  | **Value**                                       |\n|----------------------------|-------------------------------------------------|\n| Bucket name                |   astroshop_paymentservice_365                  |\n| Display name               |   AstroShop PaymentService Logs for 365 Days    |\n| Bucket table type          |   logs                                          |\n| Retention period (in days) |   365                                           |\n\nObserve and Troubleshoot for 95 Days\n\n| **Field**                  | **Value**                                         |\n|----------------------------|---------------------------------------------------|\n| Bucket name                |   astroshop_observe_and_troubleshoot_95           |\n| Display name               |   AstroShop Observe and Troubleshoot for 95 Days  |\n| Bucket table type          |   logs                                            |\n| Retention period (in days) |   95                                              |\n\nDebugging for 10 Days\n\n| **Field**                  | **Value**                            |\n|----------------------------|--------------------------------------|\n| Bucket name                |   astroshop_debugging_10             |\n| Display name               |   AstroShop Debugging for 10 Days    |\n| Bucket table type          |   logs                               |\n| Retention period (in days) |   10                                 |"
        },
        {
            "id": "3ba15da7-fd13-40fb-ac44-003ba112dde2",
            "type": "markdown",
            "markdown": "#### Step 2: Create DPL to extract important data to fields.\n#\nAfter you ingest some logs you can use a notebook, extract fields to get the DPL that you are going to use in your pipeline.\n#\nFor the `cartservice` logs, the following DPL should work to extract product and price:\n#\n`| parse content, \"DATA 'AddItemAsync' DATA 'userId='LD:userId', productId='LD:productId', quantity='INT:quantity\"`\n"
        },
        {
            "id": "cf53b04c-f3c8-4428-a37e-94ff19e42640",
            "type": "dql",
            "showTitle": false,
            "drilldownPath": [],
            "filterSegments": [],
            "previousFilterSegments": [],
            "state": {
                "input": {
                    "timeframe": {
                        "from": "now()-2h",
                        "to": "now()"
                    },
                    "value": "fetch logs\n| filter matchesValue(k8s.namespace.name,\"astroshop\") and matchesValue(k8s.container.name,\"cartservice\") and  matchesPhrase(content, \"AddItemAsync\")\n| parse content, \"DATA 'AddItemAsync' DATA 'userId='LD:userId', productId='LD:productId', quantity='INT:quantity\"\n| fields userId, productId, quantity"
                },
                "visualizationSettings": {
                    "chartSettings": {}
                },
                "querySettings": {
                    "maxResultRecords": 1000,
                    "defaultScanLimitGbytes": 500,
                    "maxResultMegaBytes": 1,
                    "defaultSamplingRatio": 10,
                    "enableSampling": false
                },
                "davis": {
                    "includeLogs": true,
                    "davisVisualization": {
                        "isAvailable": true
                    }
                },
                "state": "idle"
            }
        },
        {
            "id": "dea0a240-4526-4b69-a6d5-d971958a01ce",
            "type": "markdown",
            "markdown": "#### Step 3: Create a new pipeline for AstroShop.\n#\nPrerequisites for OpenPipeline app:\n`openpipeline:configurations:write` and `openpipeline:configurations:read` permissions\n#\nGo to Settings > Process and contextualize > OpenPipeline > Logs\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_settings_openpipeline_logs.png)\n#\nOnce data is ingested and routed, OpenPipeline processing occurs in pipelines. Each pipeline contains a set of processing instructions (processors), that are executed in an ordered sequence of stages and define how you want Dynatrace to structure, separate, and store your data. After a record is processed, it's sent to storage and is available for further analysis.\n\nBy default data types are processed in dedicated built-in pipelines. You can create new custom pipelines to group processing and extraction by technology or team.\n#\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_openpipeline_stages.png)\n#\nWhen you create a pipeline, on the *Processing* Subsection you can create a processor using DQL, Add Fields, Rename Fields or Remove Fields.\n#\nIn this case, you are going to use the DQL processor, you have to define the name, the matching condition, for this example `matchesValue(k8s.container.name,\"cartservice\") and  matchesPhrase(content, \"AddItemAsync\")` and the DPL mentioned before.\n#\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_openpipeline_astroshop_processing.png)\n#\nReferences: [OpenPipeline Processing](https://docs.dynatrace.com/docs/shortlink/openpipeline-processing)\n[Configure a processing pipeline](https://docs.dynatrace.com/docs/shortlink/openpipeline-configure)* * "
        },
        {
            "id": "b24b5db9-7ffc-4a76-a837-58a1df5deb06",
            "type": "markdown",
            "markdown": "#### Step 4: Create a Business Event for AstroShop add to cart business function.\n#\nUse OpenPipeline to convert incoming logs to business events. This is useful if logs contain business-relevant information or no other ingest path for business events is available.\n#\nTo create a business event on the same pipeline, you are going to find a subsection called *Data extraction*, in this section you have to add a business event processor and configure the name, the matching condition, the event type, the event provider, and the fields to extract.\n#\nExtract a business event for users adding items to their cart.  Extract business-grade fields `productId`, `quantity`, and `userId` in addition to observability context fields `trace_id`, `span_id`, and `trace_sampled`.\n#\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_openpipeline_astroshop_business_event.png)\n#"
        },
        {
            "id": "449f2670-5c5f-4f2c-8d83-82a9d7a4bfa2",
            "type": "markdown",
            "markdown": "#### Step 5: Create a Davis Event extraction for PaymentService anomaly detection.\n#\nDavis Events are created based on keywords you would like to be alerted upon.\n#\nTo create an Event on the same pipeline, you are going to find a subsection called *Davis*, in this section you have to configure the name, the matching condition, the Event name and Event properties.\n#\nCreate a Davis Event to identify when the PaymentService logs a failure using the condition `matchesPhrase(content, \"PaymentService Fail Feature Flag Enabled\")`.\n#\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_openpipeline_astroshop_davis_event.png)"
        },
        {
            "id": "de98ba07-6b8a-4549-b00f-e0d2a3ca313f",
            "type": "markdown",
            "markdown": "#### Step 6: Create a Metric extraction for logs by log level.\n#\nTo create a metric on the same pipeline, you are going to find a subsection called *Metric extraction*, in this section you have to configure the name, the matching condition, the metric key and the dimensions.\n#\nExtract a metric to count the number of log records by `loglevel`.  Use the condition `isNotNull(loglevel)` and the metric key `log.loglevelcounter`.  Make sure to add the dimensions `loglevel` and `status` to split the metric value by these fields.\n#\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_openpipeline_astroshop_counter_metric.png)\n#\n[Create a metric in a pipeline](https://docs.dynatrace.com/docs/platform/openpipeline/use-cases/tutorial-log-processing-pipeline#openpipeline)"
        },
        {
            "id": "88d1d5e9-43cf-4759-84a5-3837358285fd",
            "type": "markdown",
            "markdown": "#### Step 7: Create Storage Assignment Rules.\n#\nTo create a Storage Assignment rule on the same pipeline, you are going to find a subsection called *Storage*, in this section you have to configure the name, the matching condition, the Rule name and select the relevant bucket from dropdown.\n#\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_openpipeline_astroshop_storage_bucket.png)\n#\n**AstroShop Log Storage Assignment**\n\nPaymentService Logs for 365 Days\n\n| **Field**                  | **Value**                                                                                              |\n|----------------------------|--------------------------------------------------------------------------------------------------------|\n| Bucket name                |   astroshop_paymentservice_365                                                                         |\n| Display name               |   AstroShop PaymentService Logs for 365 Days                                                           |\n| Matching condition         |   matchesValue(k8s.namespace.name, \"astroshop\") AND matchesValue(k8s.container.name, \"paymentservice\") |\n\nObserve and Troubleshoot for 95 Days\n\n| **Field**                  | **Value**                                                                                               |\n|----------------------------|---------------------------------------------------------------------------------------------------------|\n| Bucket name                |   astroshop_observe_and_troubleshoot_95                                                                 |\n| Display name               |   AstroShop Observe and Troubleshoot for 95 Days                                                        |\n| Matching condition         |   matchesValue(status,\"ERROR\") OR matchesValue(status,\"WARN\") OR matchesValue(status,\"INFO\")            |\n\nDebugging for 10 Days\n\n| **Field**                  | **Value**                                                                                                |\n|----------------------------|----------------------------------------------------------------------------------------------------------|\n| Bucket name                |   astroshop_debugging_10                                                                                 |\n| Display name               |   AstroShop Debugging for 10 Days                                                                        |\n| Matching condition         |   matchesValue(status,\"DEBUG\") OR matchesValue(status,\"NONE\")                                            |\n\n#\n**Warning:** Make sure the storage assignment processor rules are in the correct order/sequence!"
        },
        {
            "id": "bbdb3a3a-53f6-438d-8896-fd33521ef5e5",
            "type": "markdown",
            "markdown": "#### Step 8: Create a Dynamic Route for new pipeline.\n#\nFinally, the last step is to configure in the `Dynamic Routing` section, the dynamic route associated to the pipeline created before, here you have to define the name, the matching condition and the target pipeline.\n#\n![](https://raw.githubusercontent.com/dynatrace-wwse/workshop-dynatrace-log-analytics/refs/heads/main/assets/images/configure-dynatrace_openpipeline_astroshop_dynamic_route.png)\n#\n[Create a dynamic route](https://docs.dynatrace.com/docs/shortlink/openpipeline-configure#route)"
        },
        {
            "id": "76333d7c-0933-4d8b-8c5e-41c0dc2fb489",
            "type": "markdown",
            "markdown": "#### Step 9: Verify pipeline created and metric extraction is working.\n#\nTo verify the Pipeline, you can search for the metric key ``log.loglevelcounter`` in the below \"Explore Metrics\" section "
        },
        {
            "id": "61b051c5-d0af-4595-8c6b-f4daf4baaec9",
            "type": "dql",
            "title": "Explore metrics",
            "showTitle": true,
            "queryConfig": {
                "version": "16.3.1",
                "subQueries": [
                    {
                        "id": "A",
                        "isEnabled": true,
                        "datatype": "metrics",
                        "metric": {
                            "key": "log.loglevelcounter",
                            "aggregation": "sum"
                        },
                        "filter": ""
                    }
                ]
            },
            "drilldownPath": [],
            "previousQueryConfig": {
                "version": "13.3.1",
                "subQueries": [
                    {
                        "id": "A",
                        "isEnabled": true,
                        "datatype": "metrics",
                        "metric": {
                            "key": "log.loglevelcounter.oteldemo",
                            "aggregation": "sum"
                        },
                        "by": [
                            "loglevel",
                            "k8s.workload.name",
                            "dt.metrics.source",
                            "metric.key",
                            "dt.system.bucket"
                        ],
                        "filter": ""
                    }
                ]
            },
            "filterSegments": [],
            "previousFilterSegments": [],
            "state": {
                "input": {
                    "timeframe": {
                        "from": "now()-2h",
                        "to": "now()"
                    },
                    "value": "timeseries { sum(log.loglevelcounter), value.A = avg(log.loglevelcounter, scalar: true) }"
                },
                "visualizationSettings": {
                    "chartSettings": {}
                },
                "querySettings": {
                    "maxResultRecords": 1000,
                    "defaultScanLimitGbytes": 500,
                    "maxResultMegaBytes": 1,
                    "defaultSamplingRatio": 10,
                    "enableSampling": false
                },
                "davis": {
                    "includeLogs": true,
                    "davisVisualization": {
                        "isAvailable": true
                    }
                },
                "state": "idle"
            }
        }
    ]
}