{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"1. About","text":"<p>Under Construction</p> <p>This guide is under construction and is not ready for use!</p> <p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"#lab-overview","title":"Lab Overview","text":"<p>During this hands-on training lab, ...</p> <p>Lab tasks:</p> <ol> <li> <p>Task 1</p> <ul> <li>Task Step or Description</li> </ul> </li> <li> <p>Task 2</p> </li> <li> <p>Task 3</p> </li> <li> <p>Clean up GitHub codespaces instance</p> </li> </ol>"},{"location":"#technical-specification","title":"Technical Specification","text":""},{"location":"#technologies-used","title":"Technologies Used","text":"<ul> <li>Dynatrace</li> <li>Kubernetes Kind<ul> <li>tested on Kind tag 0.27.0</li> </ul> </li> <li>Cert Manager - *prerequisite for OpenTelemetry Operator<ul> <li>tested on cert-manager v1.15.3</li> </ul> </li> <li>Dynatrace Operator<ul> <li>tested on v1.4.2 (April 2025)</li> </ul> </li> <li>Dynatrace OneAgent<ul> <li>tested on v1.309 (April 2025)</li> </ul> </li> </ul>"},{"location":"#reference-architecture","title":"Reference Architecture","text":""},{"location":"#continue","title":"Continue","text":"<p>In the next section, we'll review the prerequisites for this lab needed before launching our Codespaces instance.</p> <ul> <li>Continue to getting started</li> </ul>"},{"location":"2-getting-started/","title":"2. Getting started","text":"<p>Under Construction</p> <p>This guide is under construction and is not ready for use!</p> <p>Requirements</p> <ul> <li>A Dynatrace SaaS Tenant with DPS license (sign up here)<ul> <li>Live, Sprint, or Dev environment</li> <li>Full administrator access to the account and tenant</li> </ul> </li> <li>A GitHub account to interact with the demo repository and run a Codespaces instance<ul> <li>Codespaces core-hours and storage available (GitHub Billing &amp; Licensing)</li> </ul> </li> </ul>"},{"location":"2-getting-started/#prerequisites","title":"Prerequisites","text":"<p>You will need full administrator access to a Dynatrace SaaS tenant with a DPS license.</p> <ul> <li>Identify Dynatrace Tenant URL</li> <li>Identify Dynatrace Tenant Platform URL</li> <li>Generate Dynatrace Platform Token</li> </ul>"},{"location":"2-getting-started/#identify-dynatrace-tenant-url","title":"Identify Dynatrace Tenant URL","text":"<p>Identify and save/store your Dynatrace Tenant URL for the Dynatrace SaaS tenant:</p> <p>No Trailing Slash</p> <p>Do not include a trailing slash!</p> Type URL Pattern Live (Prod) https://{your-environment-id}.live.dynatrace.com Stage https://{your-environment-id}.sprint.dynatracelabs.com"},{"location":"2-getting-started/#identify-dynatrace-tenant-platform-url","title":"Identify Dynatrace Tenant Platform URL","text":"<p>The Dynatrace platform provides a collection of so-called platform services where each has its specific area of responsibility. You can access platform services in different ways. Platform services are individually versioned and offer RESTful APIs via HTTP requests.</p> <p>Dynatrace Documentation</p> <p>Identify and save/store your Dynatrace Tenant Platform URL for the Dynatrace SaaS tenant:</p> <p>No Trailing Slash</p> <p>Do not include a trailing slash!</p> Type URL Pattern Live (Prod) https://{your-environment-id}.apps.dynatrace.com Stage https://{your-environment-id}.sprint.apps.dynatracelabs.com"},{"location":"2-getting-started/#generate-dynatrace-platform-token","title":"Generate Dynatrace Platform Token","text":"<p>Use Dynatrace Account Management to create a Platform Token.</p> <p></p> <p>Dynatrace Documentation</p> <p>Generate a new Platform Token with the following scopes:</p> <pre><code>document:documents:write\ndocument:documents:read\ndocument:documents:delete\ndocument:trash.documents:delete\napp-engine:apps:run\napp-engine:functions:run\n</code></pre>"},{"location":"2-getting-started/#prerequisite","title":"Prerequisite","text":"<p>Prerequisite Details</p> <p></p>"},{"location":"2-getting-started/#continue","title":"Continue","text":"<p>In the next section, we'll launch our Codespaces instance.</p> <ul> <li>Continue to Codespaces</li> </ul>"},{"location":"3-codespaces/","title":"3. Codespaces","text":"<p>Under Construction</p> <p>This guide is under construction and is not ready for use!</p>"},{"location":"3-codespaces/#create-codespace","title":"Create Codespace","text":"<p>Click to open Codespaces for this workshop repository:</p> <p></p> <p>Codespace Configuration</p> <ul> <li>Branch<ul> <li>select the main branch</li> </ul> </li> <li>Dev container configuration<ul> <li>select Dynatrace Enablement Container</li> </ul> </li> <li>Machine type<ul> <li>select 4-core</li> </ul> </li> <li>Region<ul> <li>select any region, preferably one closest to your Dynatrace tenant</li> </ul> </li> </ul>"},{"location":"3-codespaces/#wait-for-codespace","title":"Wait for Codespace","text":"<p>We know your time is very valuable. This codespace takes around 7-10 minutes to be fully operational. A local Kubernetes (kind) cluster will be configured and in it a sample application, AstroShop, will be deployed. To make your experience better, we are also installing and configuring tools like:</p> <p>k9s kubectl helm node jq python3 gh</p>"},{"location":"3-codespaces/#deploy-demo-applications","title":"Deploy Demo Applications","text":""},{"location":"3-codespaces/#astroshop-opentelemetry-demo-app","title":"AstroShop (OpenTelemetry Demo App)","text":"<pre><code>deployAstroshop\n</code></pre>"},{"location":"3-codespaces/#easytrade","title":"EasyTrade","text":"<pre><code>deployEasyTrade\n</code></pre>"},{"location":"3-codespaces/#hipstershop","title":"HipsterShop","text":"<pre><code>deployHipsterShop\n</code></pre>"},{"location":"3-codespaces/#troubleshooting","title":"Troubleshooting","text":""},{"location":"3-codespaces/#astroshop","title":"AstroShop","text":"<p>If you encounter problems with the AstroShop app deployed in the <code>astroshop</code> namespace, you can easily recycle the pods.</p> <p>Recycle pods: <pre><code>kubectl delete pods --all -n astroshop\n</code></pre></p> <p>But before doing so, if you want to see what is happening we recommend the following: </p> <p>Verify all astroshop pods <pre><code>kubectl get pods -n astroshop\n</code></pre></p> <p>Check for events in the astroshop namespace <pre><code>kubectl get events -n astroshop\n</code></pre></p> <p>Check for system and cluster events  <pre><code>kubectl get events -n kube-system\nkubectl get events -n default\n</code></pre></p>"},{"location":"3-codespaces/#app-exposure","title":"App exposure","text":"<p>The Astroshop application is exposed via NodePort and it's mapping port 8080 to Cluster port 30100.</p> <p>Verify service: <pre><code>kubectl get svc astroshop-frontendproxy -n astroshop\n</code></pre></p>"},{"location":"3-codespaces/#deploy-dynatrace-configurations-with-monaco","title":"Deploy Dynatrace Configurations with Monaco","text":"<p>This workshop includes multiple Dynatrace configurations, such as Launchpads, Dashboards, and Notebooks.  These can be deployed to your Dynatrace tenant automatically, using Monaco.</p> <p>Start by setting your environment variables for Monaco, these will allow Monaco to authenticate with the Dynatrace Platform Services API.</p> <pre><code>export DT_PLATFORM_URL=https://{your-environment-id}.apps.dynatrace.com\nexport DT_PLATFORM_TOKEN=dt0sXX.ABC123XYZ\n</code></pre> <p>Deploy the Dynatrace configurations with Monaco using the provided helper function.</p> <pre><code>deployDynatraceConfig\n</code></pre> <p>Review the Monaco logs output in the console and check for any error messages.  If the configurations were deployed successfully, you should see Successfully deployed Dynatrace Configurations with Monaco.</p> <p></p> <p>In your Dynatrace tenant, open the Notebooks App.  Locate the newly uploaded Notebook titled <code>Workshop - Workshop Exercises</code>.</p> <p></p>"},{"location":"3-codespaces/#continue","title":"Continue","text":"<p>In the next section, we'll deploy Dynatrace on Kubernetes for full stack observability with log analytics.</p> <ul> <li>Continue to Deploy Dynatrace</li> </ul>"},{"location":"4-deploy-dynatrace/","title":"Deploy Dynatrace","text":"<p>Deploy Dynatrace Approach</p> <p>This guide offers (2) approaches to deploying Dynatrace for this workshop.  To speed things up and get log data into Dynatrace as quickly as possible, the automated (scripted) approach will deploy Dynatrace for you using helper functions.  To learn the process of deploying Dynatrace on Kubernetes and to customize the deployment to your needs, you can follow the manual (guided) approach.</p> <p>Dynatrace provides integrated log management and analytics for your Kubernetes environments by either running the OneAgent Log Module or integrating with log collectors such as Fluent Bit, OpenTelemetry Collector, Logstash, or Fluentd.</p> <p>Dynatrace provides a flexible approach to Kubernetes observability where you can pick and choose the level of observability you need for your Kubernetes clusters. The Dynatrace Operator manages all the components needed to get the data into Dynatrace for you. This also applies to collecting logs from Kubernetes containers. Depending on the selected observability option, the Dynatrace Operator configures and manages the Log Module to work in conjunction with or without a OneAgent on the node.</p> <ul> <li>Learn More</li> </ul>"},{"location":"4-deploy-dynatrace/#kubernetes-platform-monitoring-application-observability","title":"Kubernetes Platform Monitoring + Application Observability","text":"<p>Kubernetes platform monitoring sets the foundation for understanding and troubleshooting your Kubernetes clusters. This setup does not include OneAgent or application-level monitoring by default, but it can be combined with other monitoring and injection approaches.</p> <p>Kubernetes Platform Monitoring: Capabilities</p> <ul> <li>Provides insights into the health and utilization of your Kubernetes clusters, including object relationships (topology)</li> <li>Uses the Kubernetes API and cAdvisor to get node- and container-level metrics and Kubernetes events</li> <li>Enables out-of-the-box alerting and anomaly detection for workloads, Pods, nodes, and clusters</li> </ul> <p>Application observability focuses on monitoring application-level metrics by injecting code modules into application Pods. This mode offers multiple injection strategies (automatic, runtime, and build-time) to collect application-specific metrics. For infrastructure-level insights, combine it with Kubernetes platform monitoring.</p> <p>Application Observability: Capabilities</p> <ul> <li>Dynatrace injects code modules into Pods using the Kubernetes admission controller.</li> <li>Get granular control over the instrumented Pods using namespaces and annotations.</li> <li>Route Pod metrics to different Dynatrace environments within the same Kubernetes cluster.</li> <li>Enable data enrichment for Kubernetes environments.</li> </ul>"},{"location":"4-deploy-dynatrace/#automated-scripted-approach","title":"Automated (Scripted) Approach","text":""},{"location":"4-deploy-dynatrace/#generate-dynatrace-tokens","title":"Generate Dynatrace Tokens","text":"<p>In your Dynatrace tenant, launch the <code>Kubernetes</code> app.  From the Overview tab, click on <code>Add cluster</code>.</p> <p></p> <p>We do not need to configure anything on this screen other than generating the tokens.  Scroll down to the section Install Dynatrace Operator.</p> <p></p> <p>Generate a Dynatrace Operator Token.  Copy and save the value somewhere, we will need it shortly.</p> <p>Generate a Data Ingest Token.  Copy and save the value somewhere, we will need it shortly.</p>"},{"location":"4-deploy-dynatrace/#set-environment-variables","title":"Set Environment Variables","text":"<p>The automation script will utilize environment variables for deploying Dynatrace on Kubernetes.</p> <p>From your terminal, set the environment variables with the <code>export</code> command.</p> <pre><code>export DT_TENANT=https://abc123.live.dynatrace.com\nexport DT_OPERATOR_TOKEN=dt0c01.&lt;YOUR-DYNATRACE-OPERATOR-TOKEN&gt;\nexport DT_INGEST_TOKEN=dt0c01.&lt;YOUR-DATA-INGEST-TOKEN&gt;\n</code></pre>"},{"location":"4-deploy-dynatrace/#deploy-dynatrace_1","title":"Deploy Dynatrace","text":"<p>Deploy the Dynatrace Operator.</p> <pre><code>dynatraceDeployOperator\n</code></pre> <p>Validate that the Dynatrace Operator was deployed successfully.</p> <pre><code>kubectl get pods -n dynatrace\n</code></pre> <p>Deploy the Dynakube with Kubernetes Platform Monitoring + Application Observability.</p> <pre><code>deployApplicationMonitoring\n</code></pre> <p>Validate that the Dynatrace Dynakube was deployed successfully.</p> <pre><code>kubectl get pods -n dynatrace\n</code></pre>"},{"location":"4-deploy-dynatrace/#refresh-application-pods","title":"Refresh Application Pods","text":"<p>Now that Dynatrace is deployed, let's refresh/recycle the application pods for <code>astroshop</code> to inject the OneAgent code modules.</p> <pre><code>kubectl delete pods -n astroshop --field-selector=\"status.phase=Running\"\n</code></pre>"},{"location":"4-deploy-dynatrace/#validate-log-ingest","title":"Validate Log Ingest","text":"<p>In your Dynatrace tenant, return to the <code>Kubernetes</code> app.  From the Cluster overview tab, click on <code>Namespaces</code> to open the list of Namespaces on the Cluster.</p> <p></p> <p>From the list of Namespaces, click on <code>astroshop</code>.  From the Namespace pop-out, click the <code>Logs</code> tab.  Verify in the chart that logs are being ingested for the <code>astroshop</code> namespace.  Click on <code>Run query</code> on the Show logs in current context option.</p> <p></p> <p>Validate log data after running the query.</p> <p></p>"},{"location":"4-deploy-dynatrace/#continue","title":"Continue","text":"<p>In the next section, we'll learn how to scale your log analytics with Dynatrace.</p> <ul> <li>Continue to Scaling Log Analytics with Dynatrace</li> </ul>"},{"location":"4-deploy-dynatrace/#manual-guided-approach","title":"Manual (Guided) Approach","text":""},{"location":"4-deploy-dynatrace/#start-monitoring-kubernetes","title":"Start Monitoring Kubernetes","text":"<p>In your Dynatrace tenant, launch the <code>Kubernetes</code> app.  From the Overview tab, click on <code>Add cluster</code>.</p> <p></p> <p>1. Select distribution</p> <p>Choose <code>Other distributions</code> as your distribution, as we will be deploying Dynatrace on a generic Kind Kubernetes cluster.</p> <p>2. Select observability options</p> <p>Choose <code>Kubernetes platform monitoring + Application observability</code> as your observability option.  This will define your Dynakube spec/configuration.</p> <p>Toggle the <code>Log Management and Analytics</code> flag/setting to <code>Enabled</code>.</p> <p>Toggle the <code>Extensions</code> flag/setting to <code>Disabled</code>.  We will not be using this feature in this lab.</p> <p>Toggle the <code>Telemetry endpoints for data ingest</code> flag/setting to <code>Disabled</code>.  We will not be using this feature in this lab.</p> <p></p> <p>3. Configure cluster</p> <p>Give your Kubernetes cluster a name, for example <code>workshop-log-analytics</code>.</p> <p>4. Install Dynatrace Operator</p> <p>Generate a Dynatrace Operator Token.  Copy and save the value somewhere, in case you need it.  The value will automatically be added to the <code>dynakube.yaml</code> file.</p> <p>Generate a Data Ingest Token.  Copy and save the value somewhere, in case you need it.  The value will automatically be added to the <code>dynakube.yaml</code> file.</p> <p>Download the <code>dynakube.yaml</code> file.</p> <p>Copy the <code>helm install dynatrace-operator</code> command to your clipboard.  Use the command from your Dynatrace tenant, but it should look similar to this: <pre><code>helm install dynatrace-operator oci://public.ecr.aws/dynatrace/dynatrace-operator \\\n--create-namespace \\\n--namespace dynatrace \\\n--atomic\n</code></pre></p> <p></p>"},{"location":"4-deploy-dynatrace/#deploy-dynatrace-operator","title":"Deploy Dynatrace Operator","text":"<p>Navigate back to your GitHub Codespaces instance.  From the terminal, paste the <code>helm install dynatrace-operator</code> command and execute it.</p> <p></p> <p>Validate the new Dynatrace pods are running: <pre><code>kubectl get pods -n dynatrace\n</code></pre></p>"},{"location":"4-deploy-dynatrace/#deploy-dynakube","title":"Deploy Dynakube","text":"<p>Locate the <code>dynakube.yaml</code> file that you downloaded from your tenant.  With the file (directory) open, navigate back to your GitHub Codespaces instance.  Click and hold to drag and drop the <code>dynakube.yaml</code> file into your Codespaces instance.</p> <p>ActiveGate Container Resources</p> <p>Consider changing the ActiveGate's resources for better performance in this lab environment <pre><code>kind: DynaKube\nspec:\n  activeGate:\n    resources:\n      requests:\n        cpu: 100m\n        memory: 512Mi\n      limits:\n        cpu: 500m\n        memory: 768Mi\n</code></pre></p> <p>Deploy the Dynakube using <code>kubectl</code>. <pre><code>kubectl apply -f dynakube.yaml\n</code></pre></p> <p>Wait 3-5 minutes and validate that the Dynatrace pods are running. <pre><code>kubectl get pods -n dynatrace\n</code></pre></p> NAME READY STATUS RESTARTS AGE dynatrace-oneagent-csi-driver-7b9kx 4/4 Running 0 3m5s dynatrace-operator-747d795b5c-hrmtl 1/1 Running 0 3m5s dynatrace-webhook-5b697d4b9d-6v95s 1/1 Running 0 3m5s dynatrace-webhook-5b697d4b9d-nvslc 1/1 Running 0 3m5s workshop-log-analytics-activegate-0 1/1 Running 0 90s workshop-log-analytics-logmonitoring-dxrsh 1/1 Running 0 89s"},{"location":"4-deploy-dynatrace/#dynakube-log-module-spec","title":"Dynakube Log Module Spec","text":"<p>Enabling Log Management and Analytics with the option <code>Fully managed with Dynatrace Log Module</code> will add the Log Module to the Dynakube spec.</p> Dynakube sample - Kubernetes platform monitoring + Application Observability + Log Management enabled<pre><code>---\napiVersion: dynatrace.com/v1beta5\nkind: DynaKube\nmetadata:\n  name: workshop-log-analytics\n  namespace: dynatrace\n  annotations:\n    feature.dynatrace.com/k8s-app-enabled: \"true\"\nspec:\n  apiUrl: https://&lt;tenant&gt;/api\n  metadataEnrichment:\n    enabled: true\n  oneAgent:\n    applicationMonitoring: {}\n  activeGate:\n    capabilities:\n      - routing\n      - kubernetes-monitoring\n    resources:\n      requests:\n        cpu: 100m\n        memory: 512Mi\n      limits:\n        cpu: 500m\n        memory: 768Mi\n  templates:\n    logMonitoring:\n      imageRef:\n        repository: public.ecr.aws/dynatrace/dynatrace-logmodule\n        tag: 1.319.83.20250909-095914\n  logMonitoring: {}\n</code></pre> <p>The Log Module runs as a container in a standalone pod (as part of a daemonset) on each node.  The <code>spec.templates.imageRef</code> defines the container image and tag to be used.</p> <pre><code>templates:\n    logMonitoring:\n      imageRef:\n        repository: public.ecr.aws/dynatrace/dynatrace-logmodule\n        tag: 1.319.83.20250909-095914\n</code></pre> <p>ImagePullBackOff Error</p> <p>In case you encounter an ImagePullBackOff error, check public.ecr.aws to make sure the container image with that tag exists.  If not, change the value to use an existing one. </p> <p>By enabling the Log Module in your <code>dynakube.yaml</code> definition, this will enable the Dynakube to add a Log Ingest rule scoped at the Cluster-level within the Dynatrace tenant.</p>"},{"location":"4-deploy-dynatrace/#refresh-application-pods_1","title":"Refresh Application Pods","text":"<p>Now that Dynatrace is deployed, let's refresh/recycle the application pods for <code>astroshop</code> to inject the OneAgent code modules.</p> <pre><code>kubectl delete pods -n astroshop --field-selector=\"status.phase=Running\"\n</code></pre>"},{"location":"4-deploy-dynatrace/#validate-log-ingest_1","title":"Validate Log Ingest","text":"<p>In your Dynatrace tenant, return to the <code>Kubernetes</code> app.  From the Cluster overview tab, click on <code>Namespaces</code> to open the list of Namespaces on the Cluster.</p> <p></p> <p>From the list of Namespaces, click on <code>astroshop</code>.  From the Namespace pop-out, click the <code>Logs</code> tab.  Verify in the chart that logs are being ingested for the <code>astroshop</code> namespace.  Click on <code>Run query</code> on the Show logs in current context option.</p> <p></p> <p>Validate log data after running the query.</p> <p></p>"},{"location":"4-deploy-dynatrace/#continue_1","title":"Continue","text":"<p>In the next section, we'll learn how to scale your log analytics with Dynatrace.</p> <ul> <li>Continue to Scaling Log Analytics with Dynatrace</li> </ul>"},{"location":"5-scaling-log-analytics/","title":"Scaling Log Analytics","text":"<p>Let\u2019s be real \u2014 logs are your lifeline. Whether you're chasing down a production bug at 2 a.m. or trying to understand why latency just spiked, you need answers fast. That\u2019s where Dynatrace comes in. It scales log analytics effortlessly, ingesting massive volumes without breaking a sweat, and delivers lightning-fast search and AI-powered context so you can go from \u201cwhat just happened?\u201d to \u201cproblem solved\u201d in minutes. No more stitching together tools or waiting on queries \u2014 just instant clarity, full-stack visibility, and the kind of speed that makes you feel like a superhero.</p> <p>Let's learn how Dynatrace is able to scale log analytics for your organization.</p> <ul> <li>Learn More</li> </ul>"},{"location":"5-scaling-log-analytics/#logs-in-context","title":"Logs in Context","text":"<p>Automatic Correlation of Logs with Traces</p> <p>Dynatrace enriches logs with metadata such as <code>trace_id</code> and <code>span_id</code>, allowing you to seamlessly link log entries to distributed traces. This contextualization helps you understand the full transaction flow and pinpoint where issues occur within a service call or user session </p> <p>Problem-Centric Log Views via Davis AI</p> <p>When Dynatrace detects a problem (e.g., performance degradation or error spikes), it automatically surfaces only the relevant log lines tied to that issue. This reduces noise and accelerates Mean-Time-to-Identify (MTTI). You can also run recommended queries like \u201cShow logs for problem\u201d or \u201cShow surrounding logs\u201d to expand your investigation</p> <p>Integrated Observability Across Metrics, Logs, and Traces</p> <p>Dynatrace provides a unified observability platform where logs are not isolated \u2014 they're integrated with metrics, traces, and topology data. This enables you to:</p> <ul> <li>Navigate from a log entry to the affected Kubernetes workload.</li> <li>View related metrics like resource usage and business KPI.</li> <li>Analyze service-level impact and user experience</li> </ul> <p>This holistic view ensures that logs are not just raw text but part of a broader diagnostic workflow.</p>"},{"location":"5-scaling-log-analytics/#openpipeline","title":"OpenPipeline","text":"<p>Customizable and Scalable Log Ingestion</p> <p>OpenPipeline is Dynatrace\u2019s flexible data processing engine that allows you to ingest, extract, and enrich log data at scale. </p> <p>You can:</p> <ul> <li>Create custom pipelines to extract relevant log fields.</li> <li>Convert log entries into actionable events (e.g., Davis events).</li> <li>Route logs dynamically based on conditions like severity or error type.</li> </ul> <p>This modular approach ensures that only meaningful data is processed and stored, optimizing performance and cost for large environments.</p> <p>Modular Data Flow Architecture</p> <p>OpenPipeline uses a modular data flow model where each stage can be independently configured. </p> <p>This enables:</p> <ul> <li>Fine-grained control over how logs are handled.</li> <li>Easy integration with external sources and destinations.</li> <li>Reusability and scalability across teams and environments.</li> </ul> <p>This architecture supports enterprise-grade customization while maintaining performance and reliability.</p> <p>Event Extraction and AI Integration</p> <p>OpenPipeline can convert log entries into other telemetry types, extract business events, and make them actionable within Dynatrace\u2019s AI engine. </p> <p>This means:</p> <ul> <li>Logs can trigger alerts and problem detection.</li> <li>Events can be correlated with metrics, traces, and topology.</li> <li>Teams can directly relate technical issues to business impact and speed up root cause analysis.</li> </ul> <p>By turning raw logs into structured, AI-ready data, OpenPipeline helps teams solve problems faster and more intelligently.</p>"},{"location":"5-scaling-log-analytics/#bucket-strategy","title":"Bucket Strategy","text":"<p>Logical Grouping of Data for Efficient Querying</p> <p>Buckets allow you to organize log data by logical boundaries such as environment, application, team, or region. </p> <p>Consider a bucket strategy by evaluating the following information:</p> <ul> <li>Ingest sources</li> <li>Estimated Ingest volume</li> <li>Retention desires</li> <li>Compliance Requirements (audit)</li> <li>Sensitive data &amp; masking</li> <li>Permission requirements</li> </ul> <p>Bucket Considerations</p> <p>Consider the following details when using buckets with your log management and analytics solution:</p> <ul> <li>There is a limit to the number of buckets per tenant, but it can be increased based on log ingest volumes.</li> <li>Buckets exist for all data types available with Grail, not just logs - traces (spans) and business events too for example.</li> <li>Creating more buckets introduces additional administration, find the right balance.</li> <li>Leverage log attributes, including those added by OpenPipeline, to store logs in the correct bucket.</li> </ul> <p>Optimized Query Execution via Bucket Targeting</p> <p>When using Dynatrace Query Language (DQL), you can specify the bucket to query, which allows Grail to:</p> <ul> <li>Automatically optimize query execution.</li> <li>Skip irrelevant data and reduce latency.</li> <li>Leverage indexing and parallel processing for faster results.</li> </ul>"},{"location":"5-scaling-log-analytics/#segments","title":"Segments","text":"<p>Logical Filtering Across Data Types</p> <p>Segments act as predefined logical filters that span across logs, traces, metrics, events, and entities. They allow you to organize observability data by dimensions such as:</p> <ul> <li>Business applications</li> <li>Organizational units</li> <li>Infrastructure components</li> </ul> <p>This logical structuring helps teams focus on relevant data, improving query efficiency and reducing noise in large-scale environments.</p> <p>Enable Reusable, Scalable Filtering</p> <p>Segments can be configured with variables, allowing dynamic filtering across multiple buckets or entities. For example:</p> <ul> <li>A segment can include a variable to filter logs by bucket name.</li> <li>Users can select one or more values at runtime, making the segment reusable across dashboards, notebooks, and workflows.</li> </ul> <p>This flexibility supports scalable log analytics by enabling consistent filtering logic across apps and teams.</p> <p>Optimize Query Performance and Consumption</p> <p>By scoping queries to specific buckets or entities using segments, Dynatrace can:</p> <ul> <li>Reduce query execution time by narrowing the search space.</li> <li>Optimize license usage by avoiding unnecessary data scans.</li> <li>Improve responsiveness in apps like Notebooks and Dashboards.</li> </ul> <p>This makes segments a powerful tool for performance tuning and cost control in enterprise-scale environments.</p>"},{"location":"5-scaling-log-analytics/#permissions","title":"Permissions","text":"<p>Granular Access Control at Multiple Levels</p> <p>Dynatrace Grail supports fine-grained permissions at the bucket, table, field, and entity levels. This allows organizations to:</p> <ul> <li>Control who can access specific log data.</li> <li>Limit visibility based on business units, environments, or data sensitivity.</li> <li>Ensure compliance with internal and external data governance policies.</li> </ul> <p>Policy-Based Management for Scalability</p> <p>Permissions in Grail are managed through IAM policies using a flexible syntax.</p> <p>For example, <code>ALLOW storage:buckets:read WHERE storage:bucket-name = \"prod_logs\"</code>.</p> <p>These policies:</p> <ul> <li>Can be reused across users and  groups, supporting dynamic boundaries</li> <li>Support conditions like <code>IN</code>, <code>STARTSWITH</code>, and <code>=</code>, enabling scalable and dynamic access control.</li> <li>Provide access to specific log data based on record-level and field-level values</li> </ul> <p>Complement Bucket and Segment Strategy</p> <p>Controlling access to logs can be accomplished via IAM policies and defining bucket level and/or record level access:</p> <ul> <li>Bucket level control is done via bucket names.</li> <li>Record level control is done via fields or dt.security.context which must be set on the records themselves and defined within the policy.</li> </ul> <p>Implement your permission strategy that aligns and complements your bucket and segment definitions.</p>"},{"location":"5-scaling-log-analytics/#transforming-logs-to-metrics-and-events","title":"Transforming Logs to Metrics and Events","text":"<p>Create Metrics from Logs</p> <p>Dynatrace, with OpenPipeline, can create metrics and aggregate metric data points from log data at ingest.  By leveraging metrics based on your log data you'll get:</p> <ul> <li>Enhanced performance through efficient storage, faster query performance, and even better scalability for large data volumes.</li> <li>Simplified alerting with easier setup, easier to generate dynamic thesholds, and easy to aggregate/summarize data.</li> </ul> <p>Create Events from Logs</p> <p>In addition to metrics, Dynatrace can extract log data into an event:</p> <ul> <li>Extract a Davis event to detect anomalies, simplify alerting, and speed up root cause analysis.</li> <li>Extract a Business Event to enable business observablity and report on business outcomes.</li> </ul> <p>Best Practices on Using Log Data</p> <p>When using data from logs on dashboards or in alerting, consider these best practices:</p> <p>Dashboards:</p> <ul> <li>Use logs when low frequency refresh is required, involve a complex DQL, join different data types, analyzing short timeframes.<ul> <li>Use metrics when high frequency refresh is required, trending over long timeframes, need to optimize query costs, to create SLOs/alerts.</li> </ul> </li> </ul> <p>Alerting:</p> <ul> <li>Use logs when infrequent ingest of log records, complex queries, detailed investigations.<ul> <li>Use metrics when frequent data points, dynamic thresholds, and performance tracking.</li> </ul> </li> </ul>"},{"location":"5-scaling-log-analytics/#continue","title":"Continue","text":"<p>Now that we better understand how Dynatrace is able to scale log analytics for your organization, let's implement these strategies and best practices with hands-on exercises.</p> <ul> <li>Continue to configuring Dynatrace</li> </ul>"},{"location":"6-configure-dynatrace/","title":"Configure Dynatrace","text":"<p>In this workshop module, we'll configure the Dynatrace tenant based on the best practices covered in scaling log analytics.</p>"},{"location":"6-configure-dynatrace/#storage-bucket-and-openpipeline","title":"Storage Bucket and OpenPipeline","text":"<p>Complete the exercises found in the Notebook <code>Workshop - Storage Bucket and OpenPipeline</code>.</p> <p></p>"},{"location":"6-configure-dynatrace/#segments","title":"Segments","text":"<p>Complete the exercises found in the Notebook <code>Workshop - Segments</code>.</p> <p></p>"},{"location":"6-configure-dynatrace/#continue","title":"Continue","text":"<p>In the next section, we'll introduce the Dynatrace Query Language and unlock power queries and analytics on your data.</p> <ul> <li>Continue to Dynatrace Query Language</li> </ul>"},{"location":"7-dql-exercises/","title":"DQL Exercises","text":"<p>Now that logs are flowing into Dynatrace from our Kubernetes environment, it's time to unlock their full potential. Enter Dynatrace Query Language (DQL) - a powerful, intuitive language purpose-built for observability at scale. With DQL, you can slice through massive volumes of log data with precision, filter by meaningful attributes, extract insights in seconds, and even correlate logs with traces and metrics\u2014all in a single query. Whether you're troubleshooting an issue, hunting for anomalies, or building dashboards, DQL makes it easy to ask complex questions and get clear answers fast. Let\u2019s dive in and see how DQL transforms raw log data into actionable intelligence.</p> <ul> <li>Learn More</li> </ul> <p>Access the hands-on exercises for DQL from the Notebook in your Dynatrace tenant.  In your Dynatrace tenant, open the Notebooks App.  Locate the Notebook titled <code>Workshop - Workshop Exercises</code>.  We will be completing the <code>DQL Exercises</code> linked Notebooks.</p> <p></p>"},{"location":"7-dql-exercises/#logs-dql-101","title":"Logs DQL 101","text":"<p>Complete the DQL exercises found in the Notebook <code>Workshop - Logs DQL 101 - Exercises</code>.</p> <p></p> <p>Reference the Notebook <code>Workshop - Logs DQL 101 - Answer Key</code> as needed or upon completion.</p>"},{"location":"7-dql-exercises/#logs-dql-102","title":"Logs DQL 102","text":"<p>Complete the DQL exercises found in the Notebook <code>Workshop - Logs DQL 102 - Exercises</code>.</p> <p></p> <p>Reference the Notebook <code>Workshop - Logs DQL 102 - Answer Key</code> as needed or upon completion.</p>"},{"location":"7-dql-exercises/#logs-dql-201","title":"Logs DQL 201","text":"<p>Complete the DQL exercises found in the Notebook <code>Workshop - Logs DQL 201 - Exercises</code>.</p> <p></p> <p>Reference the Notebook <code>Workshop - Logs DQL 201 - Answer Key</code> as needed or upon completion.</p>"},{"location":"7-dql-exercises/#log-metrics","title":"Log Metrics","text":"<p>Complete the DQL exercises found in the Notebook <code>Workshop - Log Metrics - Exercises</code>.</p> <p></p> <p>Reference the Notebook <code>Workshop - Log Metrics - Answer Key</code> as needed or upon completion.</p>"},{"location":"7-dql-exercises/#copilot-queries","title":"CoPilot Queries","text":"<p>Complete the DQL exercises found in the Notebook <code>Workshop - CoPilot - Exercises</code>.</p> <p></p> <p>Reference the Notebook <code>Workshop - CoPilot - Answer Key</code> as needed or upon completion.</p>"},{"location":"7-dql-exercises/#continue","title":"Continue","text":"<p>In the next section, we'll learn anomaly detection strategies from log data using the Davis AI.</p> <ul> <li>Continue to Anomaly Detection</li> </ul>"},{"location":"8-anomaly-detection/","title":"Anomaly Detection","text":"<p>Now that we know how to perform powerful queries on our log data, let's explore how Dynatrace detects anomalies by analyzing raw log records and converting them into metrics using OpenPipeline. Dynatrace enhances observability by transforming log data into time-series metrics, enabling anomaly detection through static thresholds, auto-adaptive baselines, and seasonal baselines. Static thresholds provide fixed limits for alerting, while auto-adaptive baselines learn and adjust to dynamic system behavior, and seasonal baselines account for recurring patterns such as daily or weekly cycles. This approach allows for proactive identification of performance issues and unusual behavior across your environment.</p> <ul> <li>Learn More</li> </ul> <p>Return to the Notebook titled <code>Workshop - Workshop Exercises</code>.  We will be completing the <code>Log Anomaly Detection</code> linked Notebooks.</p> <p></p>"},{"location":"8-anomaly-detection/#log-anomaly-baselines","title":"Log Anomaly Baselines","text":"<p>Complete the exercises found in the Notebook <code>Workshop - Log Anomaly Baselines - Exercises</code>.</p> <p></p> <p>Reference the Notebook <code>Workshop - Log Anomaly Baselines - Answer Key</code> as needed or upon completion.</p>"},{"location":"8-anomaly-detection/#davis-anomaly-detection-logs","title":"Davis Anomaly Detection - Logs","text":"<p>Complete the exercises found in the Notebook <code>Workshop - Davis Anomaly Detection - Logs</code>.</p> <p></p>"},{"location":"8-anomaly-detection/#log-metric-anomaly-baselines","title":"Log Metric Anomaly Baselines","text":"<p>Complete the exercises found in the Notebook <code>Workshop - Metric Anomaly Baselines - Exercises</code>.</p> <p></p> <p>Reference the Notebook <code>Workshop - Metric Anomaly Baselines - Answer Key</code> as needed or upon completion.</p>"},{"location":"8-anomaly-detection/#davis-anomaly-detection-metrics","title":"Davis Anomaly Detection - Metrics","text":"<p>Complete the exercises found in the Notebook <code>Workshop - Davis Anomaly Detection - Metrics</code>.</p> <p>TODO: update screenshot</p>"},{"location":"8-anomaly-detection/#continue","title":"Continue","text":"<p>In the next section, we'll cover the primary features of building dashboards in Dynatrace.</p> <ul> <li>Continue to Dashboards in Dynatrace</li> </ul>"},{"location":"9-dashboards/","title":"9. Dashboards","text":""},{"location":"9-dashboards/#dashboards","title":"Dashboards","text":"<p>Lab task description and primer</p> <ul> <li>Learn More</li> </ul>"},{"location":"9-dashboards/#lab-step","title":"Lab Step","text":"<p>Step Details</p>"},{"location":"9-dashboards/#sub-step-1","title":"Sub Step 1","text":"<p>Sub Step Details</p>"},{"location":"9-dashboards/#sub-step-2","title":"Sub Step 2","text":"<p>Sub Step Details</p>"},{"location":"9-dashboards/#continue","title":"Continue","text":"<p>In the next section, we'll ...</p> <ul> <li>Continue to ...</li> </ul>"},{"location":"cleanup/","title":"11. Cleanup","text":"<p>Deleting the codespace from inside the container</p> <p>We like to make your life easier, for convenience there is a function loaded in the shell of the Codespace for deleting the codespace, just type <code>deleteCodespace</code>. This will trigger the deletion of the codespace.</p> <p>Another way to do this is by going to https://github.com/codespaces and delete the codespace.</p> <p>You may also want to deactivate or delete the API token needed for this lab.</p>"},{"location":"content-placeholder/","title":"Content placeholder","text":""},{"location":"content-placeholder/#lab-task","title":"Lab Task","text":"<p>Lab task description and primer</p> <ul> <li>Learn More</li> </ul>"},{"location":"content-placeholder/#lab-step","title":"Lab Step","text":"<p>Step Details</p>"},{"location":"content-placeholder/#sub-step-1","title":"Sub Step 1","text":"<p>Sub Step Details</p>"},{"location":"content-placeholder/#sub-step-2","title":"Sub Step 2","text":"<p>Sub Step Details</p>"},{"location":"content-placeholder/#continue","title":"Continue","text":"<p>In the next section, we'll ...</p> <ul> <li>Continue to ...</li> </ul>"},{"location":"resources/","title":"10. Resources","text":""},{"location":"resources/#get-your-dynatrace-environment","title":"Get your Dynatrace environment","text":"<ul> <li>Create a Free Trial in Dynatrace</li> </ul>"},{"location":"resources/#documentation","title":"Documentation","text":"<ul> <li>Dynatrace documentation</li> </ul>"},{"location":"resources/#dynatrace-news","title":"Dynatrace news","text":"<ul> <li>Dynatrace Blog</li> </ul> <ul> <li>What's Next? </li> </ul>"},{"location":"whats-next/","title":"Whats next","text":"<p>More to come</p> <ul> <li>Stay tuned, more enablements are coming whith more advanced usecases...</li> </ul>"},{"location":"snippets/admonitions/","title":"Admonitions","text":"<p>Note</p> <p>This is a Note </p> <p>Abstract</p> <p>This is an abstract</p> <p>Tipp</p> <p>This is a tipp </p> <p>Success</p> <p>This is a success </p> <p>Question</p> <p>This is a success </p> <p>Failure</p> <p>This is a failure </p> <p>Danger</p> <p>This is a danger </p> <p>Info</p> <p>This is a info</p> <p>Warning</p> <p>This is a Warning </p> <p>This is an Example admonition</p> <p>This is an example</p> This is a bug and is collapsable <p>This is a bug</p>"},{"location":"snippets/disclaimer/","title":"Disclaimer","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"snippets/requirements/","title":"Requirements","text":"<p>Requirements</p> <ul> <li>A Dynatrace SaaS Tenant with DPS license (sign up here)<ul> <li>Live, Sprint, or Dev environment</li> <li>Full administrator access to the account and tenant</li> </ul> </li> <li>A GitHub account to interact with the demo repository and run a Codespaces instance<ul> <li>Codespaces core-hours and storage available (GitHub Billing &amp; Licensing)</li> </ul> </li> </ul>"},{"location":"snippets/under-construction/","title":"Under construction","text":"<p>Under Construction</p> <p>This guide is under construction and is not ready for use!</p>"},{"location":"snippets/view-code/","title":"View code","text":"<p>View the Code</p> <p>The code for this repository is hosted on GitHub. Click the \"View Code on GitHub\" link above.</p>"}]}